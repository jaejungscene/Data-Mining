{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ekaFfjOtkTvI"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark\n",
        "# !pip install -U -q PyDrive\n",
        "# !apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XiDfep0JnWlY"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import requests\n",
        "# url = \"https://learn-ap-northeast-2-prod-fleet01-xythos.content.blackboardcdn.com/5ff5398b9f3ea/9989808?X-Blackboard-Expiration=1663750800000&X-Blackboard-Signature=Eu4JdlbtDlWwlPoneIoN2mahOmFLabAcGg0gT1SCXHw%3D&X-Blackboard-Client-Id=343987&response-cache-control=private%2C%20max-age%3D21600&response-content-disposition=inline%3B%20filename%2A%3DUTF-8%27%27soc-LiveJournal1Adj.txt&response-content-type=text%2Fplain&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDmFwLW5vcnRoZWFzdC0yIkcwRQIgeTtd1WgUNAsh73ClgJjRWWN1QlO8wyv9%2Bpo5My%2FWn0ACIQDQhuExvy4FS4ZzZwTIKxEP23Qex122YSSny8UrzN4%2F3SrcBAhuEAIaDDYzNTU2NzkyNDE4MyIMEpvHKTdWnI6m%2F9OYKrkEhHXVNBKXIbpiux7kO6KEup%2BNAQFTDO1Av5wHMFV6PQcHeBoP3yQsGCL6bihcTUDbsIm0IWp2Nl0FzL9dXt5juNFkFmsc5C63Z2As7yHa7Ql7uRDHQzOqZv4Qp%2Bt5p7AQChn%2B9t7Ufyxaky0qZSRlWvU9rAkRdBjebfRHaDMxklk4mwNbATs2EprR1uxGNTzsnwH1jNsjQc4QsGwZe%2F8JgJFdu6jdln3RkotVO5Lerez2pT%2FtZ3NyQ1hARCU0Wg6GK8PznDykBtwOM3Ldo%2BB5tADNJxYkr%2F%2BzPIYgpP65DGKkFX3i2WjSJt2fYObi4mjchWwzzU1POZVNSFCt6mYGaffM2L5P7WmTX36naQ9%2B90jdHpJlfGGMjFTBrZWAvABci6K5xXb6HNBlj%2BKXPgSKehTwi%2BeCN8RNN%2BynP69nMrhu0%2BAJSW1j4AKUrXrBMupgTdWZ%2Br5WBrIqErGnEnLFyZjIKKfGXBM6FEcXjjDqWXkRIZOK7%2BGIxa9p18aSeJID%2F8XoW3%2B7WX4eG0zVHLdfWjNXzLv3AmIgb%2Fy3v8LRiIXY8T7qyARf9hT3LqzXPs1onoR7XGgJGlUdYe46vQJrqjJFlc1Tpb10EoWaZ6NNG0ZKMb9iWQ7Tf7csLcNN2%2F8fpGCXyhV4qoxyXnS4faKm0V9v4aa4edLL8e6VJpsZBnwp8pXO4LEgD%2FnCsehTJNhsE1Z0%2F9QOkLAV7XMhdfw1Q05LMqVWWEeS6%2BJ32a1VRhAwOpcoswNA8gEw76eqmQY6qQExWk3%2F2xe59c3DTzfF%2BICBgMXdaphbaIr%2B6XeVe73JYnnyR3Y40UWs7a6TMxtujFn9MBUJf9osh%2FVksv99ANm8Te31tYEr6YJLpGHhabeJiSQUFDq%2BmvQ%2B%2FxrbGRrbFk4iDae6uCY%2B1FcsTEqt9ZektVOffztOJ3R%2BkyRaRFQqlKuSsU8xqH6WkV7d0QnC2v%2FOh3dqFGHU1GLVgr1olYFeKQ8LfngJRqa2&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220921T030000Z&X-Amz-SignedHeaders=host&X-Amz-Expires=21600&X-Amz-Credential=ASIAZH6WM4PL2EZXQ5HX%2F20220921%2Fap-northeast-2%2Fs3%2Faws4_request&X-Amz-Signature=dd32388ecb8ae52a8d03b201bd91bd178549bd008305c07a744753e332e4b5cf\"\n",
        "# text = requests.get(url).content.decode(\"utf-8\")\n",
        "# with open(\"./soc-LiveJournal1Adj.txt\", \"w\") as f:\n",
        "#     f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oCNWijoqkWYE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Spark Library\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Bh7rr4YZqQ4v"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import itertools as it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OxdqYMsokZwl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initializing Spark and preparation for data analysis\n",
        "\"\"\"\n",
        "spark = SparkSession.builder.appName(\"spakr-01\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "# conf = SparkConf()\n",
        "# sc = SparkContext(conf=conf)\n",
        "# spark = SparkSession.builder.appName(\"spark-02\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGXjjHdZgQnO",
        "outputId": "5fe5a783-d189-487d-8cd9-8970c35c9231"
      },
      "outputs": [],
      "source": [
        "# # Google drive mount\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZO_XgvBikdyV"
      },
      "outputs": [],
      "source": [
        "friendLines = sc.parallelize(sc.textFile('./soc-LiveJournal1Adj.txt').collect()) # Your folder path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "I63VLA7QHst2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usr | friends\n",
            "0   | 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94\n",
            "1   | 0,5,20,135,2409,8715,8932,10623,12347,12846,13840,13845,14005,20075,21556,22939,23520,28193,29724,29791,29826,30691,31232,31435,32317,32489,34394,35589,35605,35606,35613,35633,35648,35678,38737,43447,44846,44887,49226,49985,623,629,4999,6156,13912,14248,15190,17636,19217,20074,27536,29481,29726,29767,30257,33060,34250,34280,34392,34406,34418,34420,34439,34450,34651,45054,49592\n",
            "2   | 0,117,135,1220,2755,12453,24539,24714,41456,45046,49927,6893,13795,16659,32828,41878\n",
            "3   | 0,12,41,55,1532,12636,13185,27552,38737\n",
            "4   | 0,8,14,15,18,27,72,80,15326,19068,19079,24596,42697,46126,74,77,33269,38792,38822\n",
            "5   | 0,1,20,2022,22939,23527,30257,32503,35633,41457,43262,44846,49574,31140,32828\n",
            "6   | 0,21,98,2203,3238,5040,8795,9843,9847,15294,17874,18286,18311,18320,20553,35699,35776,38736,38750,38800,543,575,11879,12682,14943,15283,18332,18560,18625,25247,33080,34412,35785,35822,42231\n",
            "7   | 0,31993,40218,40433,1357,21843\n",
            "8   | 0,4,38,46,72,85,24777,83,33380\n",
            "9   | 0,6085,18972,19269\n"
          ]
        }
      ],
      "source": [
        "# CODE HERE\n",
        "\n",
        "print(\"usr | friends\")\n",
        "for i, line in enumerate(friendLines.collect()):\n",
        "    if i == 10:\n",
        "        break\n",
        "    linelist = line.split('\\t')\n",
        "    print(f\"{linelist[0]}   | {linelist[1]}\")\n",
        "# user_friends "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_friends = friendLines.map(lambda l: l.split())\n",
        "user  = user_friends.map(lambda x: x[0])\n",
        "Users = user.collect()\n",
        "Users = map(int, Users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49995\n",
            "49124\n",
            "====================================================================================================\n",
            "0 | [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 5, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 6, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 7, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 8, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 9, 90, 91, 92, 93, 94]\n",
            "1 | [0, 10623, 12347, 12846, 135, 13840, 13845, 13912, 14005, 14248, 15190, 17636, 19217, 20, 20074, 20075, 21556, 22939, 23520, 2409, 27536, 28193, 29481, 29724, 29726, 29767, 29791, 29826, 30257, 30691, 31232, 31435, 32317, 32489, 33060, 34250, 34280, 34392, 34394, 34406, 34418, 34420, 34439, 34450, 34651, 35589, 35605, 35606, 35613, 35633, 35648, 35678, 38737, 43447, 44846, 44887, 45054, 49226, 49592, 49985, 4999, 5, 6156, 623, 629, 8715, 8932]\n",
            "2 | [0, 117, 1220, 12453, 135, 13795, 16659, 24539, 24714, 2755, 32828, 41456, 41878, 45046, 49927, 6893]\n",
            "3 | [0, 12, 12636, 13185, 1532, 27552, 38737, 41, 55]\n",
            "4 | [0, 14, 15, 15326, 18, 19068, 19079, 24596, 27, 33269, 38792, 38822, 42697, 46126, 72, 74, 77, 8, 80]\n",
            "5 | [0, 1, 20, 2022, 22939, 23527, 30257, 31140, 32503, 32828, 35633, 41457, 43262, 44846, 49574]\n",
            "6 | [0, 11879, 12682, 14943, 15283, 15294, 17874, 18286, 18311, 18320, 18332, 18560, 18625, 20553, 21, 2203, 25247, 3238, 33080, 34412, 35699, 35776, 35785, 35822, 38736, 38750, 38800, 42231, 5040, 543, 575, 8795, 98, 9843, 9847]\n",
            "7 | [0, 1357, 21843, 31993, 40218, 40433]\n",
            "8 | [0, 24777, 33380, 38, 4, 46, 72, 83, 85]\n",
            "9 | [0, 18972, 19269, 6085]\n"
          ]
        }
      ],
      "source": [
        "user_w_friends = user_friends.filter(lambda e: len(e) == 2)\n",
        "print(user_friends.count())\n",
        "print(user_w_friends.count()) # filtering Îê®.\n",
        "print(\"=\"*100)\n",
        "user_friendList = user_w_friends.map(lambda p: (int(p[0]), map(int, sorted(p[1].split(',')))))\n",
        "for i, (usr,friendlist) in enumerate(user_friendList.collect()):\n",
        "    if i==10:\n",
        "        break\n",
        "    print(f\"{i} | {list(friendlist)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 | [((1, 10), 1), ((1, 11), 1), ((1, 12), 1), ((1, 13), 1), ((1, 14), 1)]\n",
            "1 | [((0, 10623), 1), ((0, 12347), 1), ((0, 12846), 1), ((0, 135), 1), ((0, 13840), 1)]\n",
            "2 | [((0, 117), 1), ((0, 1220), 1), ((0, 12453), 1), ((0, 135), 1), ((0, 13795), 1)]\n",
            "3 | [((0, 12), 1), ((0, 12636), 1), ((0, 13185), 1), ((0, 1532), 1), ((0, 27552), 1)]\n",
            "4 | [((0, 14), 1), ((0, 15), 1), ((0, 15326), 1), ((0, 18), 1), ((0, 19068), 1)]\n",
            "49124\n"
          ]
        }
      ],
      "source": [
        "def friend_pairs_of_user(user_friendslist):\n",
        "\t\"\"\"\n",
        "\tpairing user and friends.\n",
        "\t\"\"\"\n",
        "\tfrom_user = user_friendslist[0]\n",
        "\tfriendLst = user_friendslist[1]\n",
        "\treturn [(pair_of_usersFriend, 1) for pair_of_usersFriend in it.combinations(friendLst, 2)]\n",
        "\n",
        "pairs_from_commonFriend = user_friendList.map(friend_pairs_of_user)#.flatMap(lambda x: x)\n",
        "for i, L in enumerate(pairs_from_commonFriend.collect()):\n",
        "    if i==5:\n",
        "        break\n",
        "    print(f\"{i} | {L[0:5]}\")\n",
        "print(pairs_from_commonFriend.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 | ((1, 10), 1)\n",
            "1 | ((1, 11), 1)\n",
            "2 | ((1, 12), 1)\n",
            "3 | ((1, 13), 1)\n",
            "4 | ((1, 14), 1)\n",
            "11454543\n"
          ]
        }
      ],
      "source": [
        "pairs_from_commonFriend = pairs_from_commonFriend.flatMap(lambda x: x)\n",
        "for i, L in enumerate(pairs_from_commonFriend.collect()):\n",
        "    if i==5:\n",
        "        break\n",
        "    print(f\"{i} | {L}\")\n",
        "print(pairs_from_commonFriend.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-f02cc6fedf16>:6 ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f65735525462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Spark Apriori\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;31m# apriori(SparkContext(appName=\"Spark Apriori\"), \"../data/test.dat\", \"../result/test\", 0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# apriori(SparkContext(appName=\"Spark Apriori\"), \"../data/chess.dat\", \"../result/chess\", 0.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    353\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 355\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-f02cc6fedf16>:6 "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "DEBUG = 1\n",
        "\n",
        "\n",
        "def Dprint(info):\n",
        "    if DEBUG:\n",
        "        print(info)\n",
        "\n",
        "\n",
        "def generate_next_c(f_k, k):\n",
        "    next_c = [var1 | var2 for index, var1 in enumerate(f_k) for var2 in f_k[index + 1:] if\n",
        "              list(var1)[:k - 2] == list(var2)[:k - 2]]\n",
        "    return next_c\n",
        "\n",
        "\n",
        "def generate_f_k(sc, c_k, shared_itemset, sup):\n",
        "    def get_sup(x):\n",
        "        x_sup = len([1 for t in shared_itemset.value if x.issubset(t)])\n",
        "        if x_sup >= sup:\n",
        "            return x, x_sup\n",
        "        else:\n",
        "            return ()\n",
        "\n",
        "    f_k = sc.parallelize(c_k).map(get_sup).filter(lambda x: x).collect()\n",
        "    return f_k\n",
        "\n",
        "\n",
        "def apriori(sc, f_input, f_output, min_sup):\n",
        "    # read the raw data\n",
        "    data = sc.textFile(f_input)\n",
        "    # count the total number of samples\n",
        "    n_samples = data.count()\n",
        "    # min_sup to frequency\n",
        "    sup = n_samples * min_sup\n",
        "    # split sort\n",
        "    itemset = data.map(lambda line: sorted([int(item) for item in line.strip().split(' ')]))\n",
        "    # share the whole itemset with all workers\n",
        "    shared_itemset = sc.broadcast(itemset.map(lambda x: set(x)).collect())\n",
        "    # store for all freq_k\n",
        "    frequent_itemset = []\n",
        "\n",
        "    # prepare candidate_1\n",
        "    k = 1\n",
        "    c_k = itemset.flatMap(lambda x: set(x)).distinct().collect()\n",
        "    c_k = [{x} for x in c_k]\n",
        "\n",
        "    # when candidate_k is not empty\n",
        "    while len(c_k) > 0:\n",
        "        # generate freq_k\n",
        "        Dprint(\"C{}: {}\".format(k, c_k))\n",
        "        f_k = generate_f_k(sc, c_k, shared_itemset, sup)\n",
        "        Dprint(\"F{}: {}\".format(k, f_k))\n",
        "\n",
        "        frequent_itemset.append(f_k)\n",
        "        # generate candidate_k+1\n",
        "        k += 1\n",
        "        c_k = generate_next_c([set(item) for item in map(lambda x: x[0], f_k)], k)\n",
        "\n",
        "    # output the result to file system\n",
        "    sc.parallelize(frequent_itemset, numSlices=1).saveAsTextFile(f_output)\n",
        "    sc.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(sys.argv[2]):\n",
        "        shutil.rmtree(sys.argv[2])\n",
        "    apriori(SparkContext(appName=\"Spark Apriori\"), sys.argv[1], sys.argv[2], float(sys.argv[3]))\n",
        "    # apriori(SparkContext(appName=\"Spark Apriori\"), \"../data/test.dat\", \"../result/test\", 0.5)\n",
        "    # apriori(SparkContext(appName=\"Spark Apriori\"), \"../data/chess.dat\", \"../result/chess\", 0.8)\n",
        "    # apriori(SparkContext(appName=\"Spark Apriori\"), \"../data/mushroom.dat\", \"../result/mushroom\", 0.8)\n",
        "    # apriori(SparkContext(appName=\"Spark Apriori\"), \"../data/connect.dat\", \"../result/connect\", 0.9)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
